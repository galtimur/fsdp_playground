training:
  model_name: "meta-llama/Llama-3.2-1B"
  batch_size: 4
  test_batch_size: 10
  epochs: 2
  lr: 0.0001
  gamma: 0.7
  no_cuda: False
  seed: 1
  save_model: False